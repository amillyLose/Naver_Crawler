{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, urllib.request\n",
    "import json\n",
    "import re, os\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium.webdriver import Chrome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "delay=3\n",
    "browser = Chrome()\n",
    "browser.implicitly_wait(delay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blog_crawling(keyword, start, end):\n",
    "    \n",
    "    count = 0 # 이미지 저장 수\n",
    "    \n",
    "    folder_num = 0 # 이미지 저장 폴더 수\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    for i in range(1,400,10): # 네이버 검색은 최상의 검색결과 품질을 위해 1000건 이상의 블로그 검색결과를 제공하지 않는다.\n",
    "                                      # 해당 키워드와 기간 설정 후 끝페이지 확인 후 그에 맞게 for문 설정\n",
    "        \n",
    "        base_url = 'https://search.naver.com/search.naver?where=post&query='\n",
    "    \n",
    "        period = '&st=sim&sm=tab_opt&date_from=' + start + '&date_to=' + end\n",
    "\n",
    "        empt = '&date_option=8&srchby=all&dup_remove=1&post_blogurl=blog.naver.com&post_blogurl_without=&nso=so%3Ar%2Ca%3Aall%2Cp%3Afrom' + start + 'to' + end\n",
    "\n",
    "        page_count = '&post_blogurl=blog.naver.com&post_blogurl_without=&query=다이슨&sm=tab_pge&srchby=all&st=sim&where=post&start=' + str(i)\n",
    "        # page_count에 query='해당 키워드' 설정해야함\n",
    "        search_url = base_url + keyword + period + empt + page_count\n",
    "        \n",
    "        response = requests.get(search_url) # 블로그 출처 blog.naver.com 설정된 url 주소\n",
    "\n",
    "        soup = bs(response.text, 'html.parser')\n",
    "        \n",
    "        browser.get(search_url)\n",
    "        \n",
    "        \n",
    "        for links in soup.select('li.sh_blog_top > dl'):\n",
    "            \n",
    "            blog_info = []\n",
    "            \n",
    "            post_addr = links.select('dd.txt_block a')\n",
    "            \n",
    "            post_url = post_addr[1].get('href')\n",
    "            \n",
    "            delay=3\n",
    "            browser.get(post_url)\n",
    "            browser.implicitly_wait(delay)\n",
    "            \n",
    "            browser.switch_to.frame('mainFrame') # 네이버 블로그 수집위해 frame 전환\n",
    "            \n",
    "            soup = bs(browser.page_source, 'html.parser')\n",
    "            \n",
    "            try: # img, img2 나눈 이유는 블로그마다 이미지 소스코드가 다르기 때문에\n",
    "                img = browser.find_elements_by_css_selector('div > div > a > img.se-image-resource') \n",
    "                img2 = browser.find_elements_by_css_selector('div > div > p > img._photoImage')\n",
    "\n",
    "                link = []\n",
    "\n",
    "                for i in range(len(img)): # 해당 블로그 이미지 개수만큼 추가\n",
    "                    link.append(img[i].get_attribute('src'))\n",
    "\n",
    "                link2 = []\n",
    "\n",
    "                for i in range(len(img2)): # 해당 블로그 이미지 개수만큼 추가\n",
    "                    link2.append(img2[i].get_attribute('src'))\n",
    "\n",
    "            except:\n",
    "                img = soup.find_all(\"img\")\n",
    "\n",
    "                link = []\n",
    "\n",
    "                for i in range(len(img)): # 해당 블로그 이미지 개수만큼 추가\n",
    "                    link.append(img[i].get('src'))\n",
    "                \n",
    "            os.chdir('/Users/CPB06GameN/Desktop/와이즈넛 실무/네이버 블로그 크롤링/Crawling_미국쑥부쟁이/') # 이미지 저장 폴더 경로 설정\n",
    "                             \n",
    "            folder_num += 1\n",
    "\n",
    "            try: # 폴더 생성\n",
    "                os.makedirs(os.path.join('BLOG'+str(folder_num)))\n",
    "\n",
    "            except OSError as e:\n",
    "                if e.errno != errno.EEXIST:\n",
    "                    print(\"폴더 생성 실패\")\n",
    "                    exit() \n",
    "\n",
    "            for url in link: # 이미지 1 파일 저장\n",
    "                count+=1\n",
    "                urllib.request.urlretrieve(url, 'BLOG'+str(folder_num)+'/'+'BLOG_img'+str(count)+'.jpg')  \n",
    "\n",
    "            count = 0 # 이미지 저장명 초기화\n",
    "\n",
    "            for url in link2: # 이미지 2 파일 저장\n",
    "                count+=1\n",
    "                urllib.request.urlretrieve(url, 'BLOG'+str(folder_num)+'/'+'BLOG_img'+str(count)+'.jpg') \n",
    "\n",
    "            count = 0 # 이미지 저장명 초기화   \n",
    "            \n",
    "            \n",
    "            title_info = soup.find('meta', property = 'og:title') # 글 제목 추출\n",
    "            \n",
    "            title = title_info['content']\n",
    "            \n",
    "            try: # 블로그마다 소스코드가 달라서 2가지 경우의 예외처리 추가\n",
    "                date = browser.find_element_by_css_selector('div > div > div.blog2_container > span.se_publishDate.pcol2').text # 글 쓴 날짜 및 시간 추출\n",
    "                                \n",
    "                content_tags = soup.select('div > div.se-main-container')[0].select('p') # 본문 내용 추출\n",
    "                \n",
    "                content_info = ' '.join([ tags.get_text() for tags in content_tags ])\n",
    "            \n",
    "                content = re.sub('\\u200b','',content_info)\n",
    "                \n",
    "            except:\n",
    "                \n",
    "                try:\n",
    "                    date = browser.find_element_by_css_selector('tbody > tr > td.bcc > table > tbody > tr > td > p.date.fil5.pcol2._postAddDate').text # 글 쓴 날짜 및 시간 추출\n",
    "                \n",
    "                    content_temp = soup.find(\"div\", {\"id\": \"postViewArea\"}) # 본문 내용 추출\n",
    "\n",
    "                    content_info = content_temp.text\n",
    "\n",
    "                    content = re.sub('\\u200b','',content_info)\n",
    "                    \n",
    "                except:\n",
    "                    date = browser.find_element_by_css_selector('div > div > div.blog2_container > span.se_publishDate.pcol2').text # 글 쓴 날짜 및 시간 추출\n",
    "                    \n",
    "                    content = browser.find_element_by_css_selector('div.se_component_wrap.sect_dsc.__se_component_area').text # 본문 내용 추출\n",
    "\n",
    "\n",
    "\n",
    "            blog_info = {\n",
    "                            'title' : title,\n",
    "                            'content' : content,\n",
    "                            'date' : date            \n",
    "                        }\n",
    "            \n",
    "            results.append(blog_info)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-34d20e31d7fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mresults_Print\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblog_crawling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'미국쑥부쟁이'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'20190101'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'20191025'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'naverBlog_crawling_data.json'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'w'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmake_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-8753428980fc>\u001b[0m in \u001b[0;36mblog_crawling\u001b[1;34m(keyword, start, end)\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlink\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# 이미지 1 파일 저장\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m                 \u001b[0mcount\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m                 \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'BLOG'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'BLOG_img'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;31m# 이미지 저장명 초기화\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m    278\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m                 \u001b[0mread\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 280\u001b[1;33m                 \u001b[0mtfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    281\u001b[0m                 \u001b[0mblocknum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mreporthook\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    results_Print = blog_crawling('미국쑥부쟁이','20190101','20191025')\n",
    "\n",
    "    with open('naverBlog_crawling_data.json','w',encoding = 'utf-8') as make_file:\n",
    "        json.dump(results_Print,make_file,ensure_ascii=False,indent='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
